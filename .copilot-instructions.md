# Copilot Instructions for nlp2sql

## Project Overview

**nlp2sql** is an enterprise-ready Python library that translates natural language queries to SQL using multiple AI providers (OpenAI, Anthropic Claude, Google Gemini) with a clean, scalable architecture optimized for handling large database schemas (1000+ tables).

**Current Version**: 0.2.0rc2 (Release Candidate)

## Key Architecture Patterns

- **Clean Architecture/Hexagonal Architecture**: Core business logic is isolated from external dependencies
- **Design Patterns**: Factory, Strategy, Adapter, Repository
- **SOLID Principles**: All components follow single responsibility and dependency inversion
- **Schema Management**: Multiple strategies for handling large schemas (>1000 tables) with embeddings and caching
- **Async/Await Pattern**: Full async support for better performance

## Core Features

- **Multi-Provider Support**: OpenAI, Anthropic Claude, Google Gemini (AWS Bedrock and Azure OpenAI planned)
- **Database Support**: PostgreSQL (primary), with MySQL, SQLite, Oracle, MSSQL coming soon
- **Schema Filtering**: Advanced filtering for enterprise databases with thousands of tables
- **Vector Embeddings**: Semantic search using FAISS and sentence-transformers
- **Smart Caching**: Redis and in-memory caching strategies
- **MCP Server**: Model Context Protocol server for integration with Claude Desktop

## Tracked Files and Integrations

- Main documentation: `@README.md`, `@CLI_GUIDE.md`, `@QUICK_START.md`, `@USAGE.md`
- Development guides: `@CONTRIBUTING.md`, `@PR_TEMPLATE.md`
- Cross-referencing and maintaining consistency across multiple documentation files is crucial

## Development Commands

### UV Package Management
```bash
# Initialize project
uv init

# Install all dependencies
uv sync

# Add new dependency
uv add package_name

# Add development dependency
uv add --dev package_name

# Run tests
uv run pytest

# Run linting
uv run ruff check .

# Format code
uv run ruff format .

# Type checking
uv run mypy src/

# Build package
uv build

# Run specific test
uv run pytest tests/test_basic.py -v
```

### Development Workflow
```bash
# Setup Docker test databases
cd docker && docker-compose up -d && cd ..

# Run CLI commands
uv run nlp2sql --help
uv run nlp2sql setup      # Interactive setup
uv run nlp2sql validate   # Validate configuration
uv run nlp2sql query --database-url "postgresql://testuser:testpass@localhost:5432/testdb" --question "How many users?"

# Run example scripts
uv run python examples/getting_started/basic_usage.py
uv run python examples/getting_started/simple_demo.py
uv run python examples/advanced/test_multiple_providers.py

# Test with different providers
export OPENAI_API_KEY=your-key
export ANTHROPIC_API_KEY=your-key
export GOOGLE_API_KEY=your-key  # Note: GOOGLE_API_KEY for Gemini

# Quick installation script for development
./install-dev.sh
```

## High-Level Architecture

### Core Components

1. **Ports (Interfaces)** - Located in `src/nlp2sql/ports/`
   - `AIProviderPort`: Interface for all AI providers
   - `SchemaRepositoryPort`: Interface for schema storage/retrieval
   - `CachePort`: Interface for caching mechanisms
   - `QueryOptimizerPort`: Interface for query optimization strategies
   - `SchemaStrategyPort`: Interface for schema handling strategies

2. **Adapters (Implementations)** - Located in `src/nlp2sql/adapters/`
   - `OpenAIAdapter`: OpenAI GPT-4 implementation
   - `AnthropicAdapter`: Anthropic Claude implementation
   - `GeminiAdapter`: Google Gemini implementation
   - `PostgreSQLRepository`: PostgreSQL schema repository (primary implementation)
   - Note: AWS Bedrock and Azure OpenAI adapters are planned but not yet implemented

3. **Services (Business Logic)** - Located in `src/nlp2sql/services/`
   - `QueryGenerationService`: Main service orchestrating NL to SQL conversion
   - Includes query validation, caching, and optimization

4. **Schema Management** - Located in `src/nlp2sql/schema/`
   - `SchemaManager`: Coordinates all schema strategies with filtering support
   - `SchemaAnalyzer`: Analyzes and scores schema relevance
   - `SchemaEmbeddingManager`: Creates and searches FAISS vector embeddings
   - Uses sentence-transformers (all-MiniLM-L6-v2 by default)

### Data Flow

1. User submits natural language query
2. `QueryGenerationService` receives the query
3. `SchemaManager` applies filters (include/exclude schemas, tables, system tables)
4. `SchemaEmbeddingManager` finds semantically similar schema elements using FAISS
5. `SchemaAnalyzer` scores relevance of schema elements
6. Relevant schema context is prepared within token limits
7. Selected AI provider (OpenAI/Anthropic/Gemini) generates SQL
8. Query is validated and results are cached for future use

## Critical Implementation Details

### Large Schema Handling

The library implements multiple strategies to handle schemas with 1000+ tables:

1. **Schema Filtering** (Primary Strategy)
   - `include_schemas`: Whitelist specific schemas
   - `exclude_schemas`: Blacklist schemas
   - `include_tables`: Whitelist specific tables
   - `exclude_tables`: Blacklist tables
   - `exclude_system_tables`: Filter out system/internal tables

2. **Vector Embeddings with FAISS**
   - Uses sentence-transformers for semantic similarity
   - FAISS index for efficient nearest neighbor search
   - Embeddings cached in `/tmp/nlp2sql_embeddings/` (MCP server) or project embeddings/

3. **Token Management**
   - Default max schema tokens: 8000 (configurable via `NLP2SQL_MAX_SCHEMA_TOKENS`)
   - Dynamically adjusts schema context based on provider limits
   - Prioritizes most relevant schema elements

### Provider-Specific Considerations

- **OpenAI**: GPT-4 Turbo, 128k token context, best for complex reasoning
- **Anthropic**: Claude 3 Opus, 200k token context, excellent for large schemas
- **Gemini**: Gemini Pro, 1M token context, cost-effective for high volume
- Note: AWS Bedrock and Azure OpenAI adapters are planned but not yet implemented

## Testing Strategy

- Currently minimal test coverage (`tests/test_basic.py`)
- Example scripts serve as integration tests (`examples/` directory)
- MCP server tests in `test_tmp/` directory
- Benchmarks in `benchmarks/enterprise_benchmark_results.json`
- Docker test databases for real-world testing

## Configuration

The library uses a hierarchical configuration system:
1. Default configurations in `src/nlp2sql/config/settings.py` (using Pydantic Settings)
2. Environment variables (see below for complete list)
3. Runtime parameters passed to functions

## Security Considerations

- Never log or expose API keys
- Implement query validation to prevent SQL injection
- Use parameter binding for generated queries
- Sanitize schema metadata before processing
- Implement rate limiting per user/API key

## Performance Optimization

- Schema metadata is cached aggressively
- Embeddings are pre-computed and stored
- Connection pooling for database access
- Async operations where possible
- Batch processing for multiple queries

## Environment Variables

### API Keys (at least one required)
- `OPENAI_API_KEY`: OpenAI API key
- `ANTHROPIC_API_KEY`: Anthropic API key  
- `GOOGLE_API_KEY`: Google API key for Gemini (NOT `GEMINI_API_KEY`)

### Database Configuration
- `DATABASE_URL`: Default database connection URL
- `NLP2SQL_DB_POOL_SIZE`: Connection pool size (default: 10)
- `NLP2SQL_DB_MAX_OVERFLOW`: Max overflow connections (default: 20)

### Schema Management
- `NLP2SQL_MAX_SCHEMA_TOKENS`: Max tokens for schema context (default: 8000)
- `NLP2SQL_SCHEMA_CACHE_ENABLED`: Enable schema caching (default: true)
- `NLP2SQL_SCHEMA_REFRESH_HOURS`: Schema refresh interval (default: 24)
- `NLP2SQL_EMBEDDING_MODEL`: Sentence transformer model (default: all-MiniLM-L6-v2)

### General Settings
- `NLP2SQL_DEBUG`: Enable debug mode (default: false)
- `NLP2SQL_LOG_LEVEL`: Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
- `NLP2SQL_CACHE_ENABLED`: Enable query caching (default: true)
- `NLP2SQL_CACHE_TTL`: Cache TTL in seconds (default: 3600)
- `NLP2SQL_TEMPERATURE`: LLM temperature (default: 0.1)
- `NLP2SQL_MAX_TOKENS`: Max tokens for response (default: 2000)

### MCP Server Specific
- `NLP2SQL_CACHE_DIR`: Cache directory (default: /tmp/nlp2sql_cache)
- `NLP2SQL_EMBEDDINGS_DIR`: Embeddings directory (default: /tmp/nlp2sql_embeddings)
- `NLP2SQL_DEMO_DB_URL`: Demo database URL for MCP server
- `NLP2SQL_LOCAL_DB_URL`: Local database URL for MCP server
- `NLP2SQL_TEST_DB_URL`: Test database URL for MCP server

## Common Development Tasks

### Adding a New AI Provider

1. Create adapter in `src/nlp2sql/adapters/` implementing `AIProviderPort`
2. Add provider configuration to `src/nlp2sql/config/settings.py`
3. Update `create_query_service()` in `src/nlp2sql/__init__.py`
4. Add example script in `examples/advanced/`
5. Update documentation with provider-specific details

### Working with Schema Filters

```python
# Example schema filters for large databases
schema_filters = {
    "include_schemas": ["sales", "finance"],  # Only these schemas
    "exclude_schemas": ["archive", "temp"],   # Exclude these schemas
    "include_tables": ["users", "orders"],    # Only these tables
    "exclude_tables": ["audit_logs"],         # Exclude these tables
    "exclude_system_tables": True             # Filter system tables
}
```

### Debugging Token Limit Issues

1. Set `NLP2SQL_MAX_SCHEMA_TOKENS` to a lower value
2. Use schema filters to reduce schema size
3. Check embeddings cache in project `embeddings/` directory
4. Monitor logs with `NLP2SQL_LOG_LEVEL=DEBUG`
5. Consider using Anthropic or Gemini for larger context windows

## Error Handling Patterns

- Custom exceptions in `src/nlp2sql/exceptions/`:
  - `NLP2SQLException`: Base exception
  - `SchemaException`: Schema-related errors
  - `ProviderException`: AI provider errors
  - `TokenLimitException`: Token limit exceeded
  - `QueryGenerationException`: Query generation failures
  - `ValidationException`: Query validation errors
  - `ConfigurationException`: Configuration issues
- Retry logic with tenacity library (3 attempts by default)
- Structured logging with structlog
- Graceful fallbacks for provider failures

## Docker Test Databases

The project includes Docker Compose setup for testing:

```bash
cd docker
docker-compose up -d
```

This creates two PostgreSQL databases:
1. **Simple Database** (port 5432):
   - URL: `postgresql://testuser:testpass@localhost:5432/testdb`
   - Basic schema with users, posts, comments tables
   
2. **Enterprise Database** (port 5433):
   - URL: `postgresql://demo:demo123@localhost:5433/enterprise`
   - Large schema with 50+ tables for testing at scale

## Project Structure

```
nlp2sql/
├── src/nlp2sql/          # Main library code
│   ├── __init__.py       # Public API and helper functions
│   ├── adapters/         # AI provider implementations
│   ├── config/           # Settings and configuration
│   ├── core/             # Core entities and domain logic
│   ├── exceptions/       # Custom exceptions
│   ├── ports/            # Interface definitions
│   ├── schema/           # Schema management and embeddings
│   ├── services/         # Business logic services
│   └── utils/            # Utility functions
├── mcp_server/           # Model Context Protocol server
├── examples/             # Usage examples
├── tests/                # Test suite
├── docker/               # Docker test environment
└── docs/                 # Additional documentation
```

## Key Dependencies

- **AI/ML**: openai, anthropic, google-generativeai, sentence-transformers, faiss-cpu
- **Database**: sqlalchemy, psycopg2-binary, asyncpg
- **Core**: pydantic, pydantic-settings, structlog, tenacity
- **Development**: pytest, ruff, mypy, uv (package manager)

## Coding Standards

### Code Style
- Use Ruff for linting and formatting
- Follow Black code style (line length: 120)
- Use type hints everywhere (MyPy compliance)
- Async/await patterns for I/O operations

### Architecture Principles
- Follow Clean Architecture patterns
- Implement SOLID principles
- Use dependency injection through ports/adapters
- Separate business logic from infrastructure concerns

### Naming Conventions
- Classes: PascalCase (e.g., `QueryGenerationService`)
- Functions/methods: snake_case (e.g., `generate_query`)
- Constants: UPPER_SNAKE_CASE (e.g., `MAX_SCHEMA_TOKENS`)
- Files: snake_case (e.g., `query_generation_service.py`)

### Documentation
- Docstrings for all public APIs
- Type hints for all function parameters and returns
- Update relevant documentation files when making changes
- Keep examples/ directory updated with working code

## Common Patterns

### Service Implementation
```python
from nlp2sql.ports.ai_provider_port import AIProviderPort
from nlp2sql.core.entities import QueryRequest, QueryResult

class QueryGenerationService:
    def __init__(self, ai_provider: AIProviderPort):
        self.ai_provider = ai_provider
    
    async def generate_query(self, request: QueryRequest) -> QueryResult:
        # Implementation following Clean Architecture
        pass
```

### Adapter Implementation
```python
from nlp2sql.ports.ai_provider_port import AIProviderPort

class NewProviderAdapter(AIProviderPort):
    async def generate_sql(self, prompt: str, **kwargs) -> str:
        # Provider-specific implementation
        pass
```

### Error Handling
```python
from nlp2sql.exceptions import QueryGenerationException
from tenacity import retry, stop_after_attempt

@retry(stop=stop_after_attempt(3))
async def generate_query_with_retry(self, request: QueryRequest) -> QueryResult:
    try:
        return await self._generate_query(request)
    except Exception as e:
        raise QueryGenerationException(f"Failed to generate query: {e}")
```

## Testing Guidelines

- Write unit tests for core business logic
- Use pytest fixtures for common test data
- Mock external dependencies (AI providers, databases)
- Integration tests should use Docker test databases
- Example scripts serve as end-to-end tests

## Performance Considerations

- Use async/await for all I/O operations
- Implement caching at multiple levels (schema, embeddings, results)
- Monitor token usage across providers
- Use connection pooling for database operations
- Pre-compute embeddings for large schemas